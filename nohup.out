Ivy Default Cache set to: /home/abdul.sabbir/.ivy2/cache
The jars for the packages stored in: /home/abdul.sabbir/.ivy2/jars
:: loading settings :: url = jar:file:/usr/hdp/2.6.3.0-235/spark2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
mysql#mysql-connector-java added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found mysql#mysql-connector-java;5.1.39 in central
:: resolution report :: resolve 166ms :: artifacts dl 6ms
	:: modules in use:
	mysql#mysql-connector-java;5.1.39 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/5ms)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:ERROR Could not instantiate class [INFO].
java.lang.ClassNotFoundException: INFO
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Could not instantiate class [WARNING].
java.lang.ClassNotFoundException: WARNING
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:WARN No such property [datePattern] in org.apache.log4j.FileAppender.
log4j:ERROR Could not instantiate class [DEBUG].
java.lang.ClassNotFoundException: DEBUG
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
main function for this pyspark project is going to be initiated soon .....
current working directory is :/hdpdata/pysparkProject/dqMachine
Operating system name : Linux
Machine Name : lpwhdqdnp01.npd.com
operating system version : 3.10.0-693.5.2.el7.x86_64
last access time #1 SMP Fri Oct 13 10:46:25 EDT 2017
operating system architecture : x86_64
Usage: snapshot detector number of days: 0
2018-08-08 12:37:29,502 [Thread-3] INFO  org.apache.spark.SparkContext - Running Spark version 2.2.0.2.6.3.0-235
2018-08-08 12:37:29,750 [Thread-3] WARN  org.apache.spark.SparkConf - In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
2018-08-08 12:37:29,758 [Thread-3] INFO  org.apache.spark.SparkContext - Submitted application: dqMachineApplication
2018-08-08 12:37:29,844 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-08-08 12:37:29,844 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-08-08 12:37:29,845 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-08-08 12:37:29,846 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-08-08 12:37:29,846 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-08-08 12:37:30,099 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 33144.
2018-08-08 12:37:30,115 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2018-08-08 12:37:30,133 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2018-08-08 12:37:30,135 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-08-08 12:37:30,135 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2018-08-08 12:37:30,143 [Thread-3] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /hdpdata/tmp/blockmgr-07cea740-85ef-4a1a-8bd1-d7d0b8ec0466
2018-08-08 12:37:30,162 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 16.8 GB
2018-08-08 12:37:30,281 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2018-08-08 12:37:30,352 [Thread-3] INFO  org.spark_project.jetty.util.log - Logging initialized @2527ms
2018-08-08 12:37:30,407 [Thread-3] INFO  org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
2018-08-08 12:37:30,419 [Thread-3] INFO  org.spark_project.jetty.server.Server - Started @2596ms
2018-08-08 12:37:30,433 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2018-08-08 12:37:30,434 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
2018-08-08 12:37:30,435 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
2018-08-08 12:37:30,436 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
2018-08-08 12:37:30,441 [Thread-3] INFO  org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@295ace64{HTTP/1.1,[http/1.1]}{0.0.0.0:4044}
2018-08-08 12:37:30,442 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4044.
2018-08-08 12:37:30,463 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d6f1f9b{/jobs,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,464 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@8b5de63{/jobs/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,465 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@179ba27{/jobs/job,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,466 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33d5317a{/jobs/job/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,466 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47339a4{/stages,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,467 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@47bc8cc0{/stages/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,468 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2db4dc08{/stages/stage,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,469 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12c058a4{/stages/stage/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,470 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b5393b7{/stages/pool,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,470 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e24fe4f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,471 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@b6a479a{/storage,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,472 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30341efa{/storage/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,472 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1875b19f{/storage/rdd,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,473 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b468a50{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,474 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ca787fe{/environment,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,474 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64ce4eee{/environment/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,475 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@340dbe77{/executors,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,476 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13a85795{/executors/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,476 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46fe5a3{/executors/threadDump,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,477 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e11f133{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,482 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@84e7864{/static,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,483 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ae4f2a1{/,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,484 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2f98ae41{/api,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,485 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13aaefc0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,485 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7b85d984{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-08-08 12:37:30,487 [Thread-3] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.231.1.14:4044
2018-08-08 12:37:30,556 [Thread-3] WARN  org.apache.spark.scheduler.FairSchedulableBuilder - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
2018-08-08 12:37:30,559 [Thread-3] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
2018-08-08 12:37:31,311 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Looking for the active RM in [rm1, rm2]...
2018-08-08 12:37:31,370 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Found active RM [rm1]
2018-08-08 12:37:31,371 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Requesting a new application from cluster with 10 NodeManagers
2018-08-08 12:37:31,416 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Verifying our application has not requested more than the maximum memory capability of the cluster (196608 MB per container)
2018-08-08 12:37:31,416 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Will allocate AM container, with 896 MB memory including 384 MB overhead
2018-08-08 12:37:31,417 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up container launch context for our AM
2018-08-08 12:37:31,421 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up the launch environment for our AM container
2018-08-08 12:37:31,428 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Preparing resources for our AM container
2018-08-08 12:37:32,279 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-08-08 12:37:32,282 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Source and destination file systems are the same. Not copying hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-08-08 12:37:32,342 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_13110/mysql_mysql-connector-java-5.1.39.jar
2018-08-08 12:37:32,532 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_13110/pyspark.zip
2018-08-08 12:37:32,557 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_13110/py4j-0.10.4-src.zip
2018-08-08 12:37:32,581 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/pysparkProject/dqMachine/dq_machine.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_13110/dq_machine.zip
2018-08-08 12:37:32,604 [Thread-3] WARN  org.apache.spark.deploy.yarn.Client - Same path resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.
2018-08-08 12:37:32,629 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/tmp/spark-aee282d4-aef2-4ca5-ba79-3c109042cca9/__spark_conf__5015327215348192520.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_13110/__spark_conf__.zip
2018-08-08 12:37:32,667 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-08-08 12:37:32,668 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-08-08 12:37:32,668 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-08-08 12:37:32,668 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-08-08 12:37:32,668 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-08-08 12:37:32,674 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Submitting application application_1524081153081_13110 to ResourceManager
2018-08-08 12:37:32,702 [Thread-3] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1524081153081_13110
2018-08-08 12:37:32,704 [Thread-3] INFO  org.apache.spark.scheduler.cluster.SchedulerExtensionServices - Starting Yarn extension services with app application_1524081153081_13110 and attemptId None
2018-08-08 12:37:33,711 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_13110 (state: ACCEPTED)
2018-08-08 12:37:33,716 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1533746252684
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_13110/
	 user: abdul.sabbir
2018-08-08 12:37:34,717 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_13110 (state: ACCEPTED)
2018-08-08 12:37:35,656 [dispatcher-event-loop-38] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lpwhdqnnp01.npd.com,lpwhdqnnp02.npd.com, PROXY_URI_BASES -> http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_13110,http://lpwhdqnnp02.npd.com:8088/proxy/application_1524081153081_13110), /proxy/application_1524081153081_13110
2018-08-08 12:37:35,659 [dispatcher-event-loop-38] INFO  org.apache.spark.ui.JettyUtils - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2018-08-08 12:37:35,719 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_13110 (state: ACCEPTED)
2018-08-08 12:37:35,897 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
2018-08-08 12:37:36,721 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_13110 (state: RUNNING)
2018-08-08 12:37:36,722 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.231.1.22
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1533746252684
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_13110/
	 user: abdul.sabbir
2018-08-08 12:37:36,722 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Application application_1524081153081_13110 has started running.
2018-08-08 12:37:36,731 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35019.
2018-08-08 12:37:36,731 [Thread-3] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.231.1.14:35019
2018-08-08 12:37:36,734 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-08-08 12:37:36,736 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.231.1.14, 35019, None)
2018-08-08 12:37:36,739 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.231.1.14:35019 with 16.8 GB RAM, BlockManagerId(driver, 10.231.1.14, 35019, None)
2018-08-08 12:37:36,742 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.231.1.14, 35019, None)
2018-08-08 12:37:36,743 [Thread-3] INFO  org.apache.spark.storage.BlockManager - external shuffle service port = 7447
2018-08-08 12:37:36,743 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.231.1.14, 35019, None)
2018-08-08 12:37:36,909 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f8f6f5c{/metrics/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:37,054 [Thread-3] INFO  org.apache.spark.scheduler.EventLoggingListener - Logging events to file:/hdpdata/logs/application_1524081153081_13110
2018-08-08 12:37:39,178 [dispatcher-event-loop-30] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:54356) with ID 4
2018-08-08 12:37:39,234 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:34900) with ID 8
2018-08-08 12:37:39,245 [dispatcher-event-loop-38] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:45671 with 11.1 GB RAM, BlockManagerId(4, lpwhdqdnp07.npd.com, 45671, None)
2018-08-08 12:37:39,257 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:47434) with ID 1
2018-08-08 12:37:39,281 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:36978) with ID 2
2018-08-08 12:37:39,286 [dispatcher-event-loop-27] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:46627 with 11.1 GB RAM, BlockManagerId(8, lpwhdqdnp05.npd.com, 46627, None)
2018-08-08 12:37:39,299 [dispatcher-event-loop-36] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:46592 with 11.1 GB RAM, BlockManagerId(1, lpwhdqdnp03.npd.com, 46592, None)
2018-08-08 12:37:39,303 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:46674) with ID 6
2018-08-08 12:37:39,324 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:51934) with ID 7
2018-08-08 12:37:39,340 [dispatcher-event-loop-7] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:33593 with 11.1 GB RAM, BlockManagerId(2, lpwhdqdnp02.npd.com, 33593, None)
2018-08-08 12:37:39,345 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:35810) with ID 5
2018-08-08 12:37:39,364 [dispatcher-event-loop-21] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:37392 with 11.1 GB RAM, BlockManagerId(6, lpwhdqdnp06.npd.com, 37392, None)
2018-08-08 12:37:39,373 [dispatcher-event-loop-27] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:33441 with 11.1 GB RAM, BlockManagerId(7, lpwhdqdnp01.npd.com, 33441, None)
2018-08-08 12:37:39,378 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53402) with ID 3
2018-08-08 12:37:39,412 [dispatcher-event-loop-5] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:44109 with 11.1 GB RAM, BlockManagerId(5, lpwhdqdnp04.npd.com, 44109, None)
2018-08-08 12:37:39,435 [dispatcher-event-loop-18] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:44182 with 11.1 GB RAM, BlockManagerId(3, lpwhdqdnp08.npd.com, 44182, None)
2018-08-08 12:37:39,472 [dispatcher-event-loop-33] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:40174) with ID 9
2018-08-08 12:37:39,520 [dispatcher-event-loop-31] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:38767 with 11.1 GB RAM, BlockManagerId(9, lpwhdqdnp09.npd.com, 38767, None)
2018-08-08 12:37:39,586 [dispatcher-event-loop-13] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:47438) with ID 12
2018-08-08 12:37:39,622 [dispatcher-event-loop-25] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:34906) with ID 18
2018-08-08 12:37:39,630 [dispatcher-event-loop-32] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:46678) with ID 16
2018-08-08 12:37:39,630 [dispatcher-event-loop-21] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:38912 with 11.1 GB RAM, BlockManagerId(12, lpwhdqdnp03.npd.com, 38912, None)
2018-08-08 12:37:39,632 [dispatcher-event-loop-33] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:54362) with ID 14
2018-08-08 12:37:39,637 [dispatcher-event-loop-15] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:36982) with ID 11
2018-08-08 12:37:39,676 [dispatcher-event-loop-38] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:42689 with 11.1 GB RAM, BlockManagerId(18, lpwhdqdnp05.npd.com, 42689, None)
2018-08-08 12:37:39,681 [dispatcher-event-loop-29] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:45129 with 11.1 GB RAM, BlockManagerId(11, lpwhdqdnp02.npd.com, 45129, None)
2018-08-08 12:37:39,685 [dispatcher-event-loop-31] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:46630 with 11.1 GB RAM, BlockManagerId(16, lpwhdqdnp06.npd.com, 46630, None)
2018-08-08 12:37:39,687 [dispatcher-event-loop-37] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:44362 with 11.1 GB RAM, BlockManagerId(14, lpwhdqdnp07.npd.com, 44362, None)
2018-08-08 12:37:39,712 [dispatcher-event-loop-35] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:35814) with ID 15
2018-08-08 12:37:39,720 [dispatcher-event-loop-39] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53406) with ID 13
2018-08-08 12:37:39,759 [dispatcher-event-loop-10] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:33359 with 11.1 GB RAM, BlockManagerId(15, lpwhdqdnp04.npd.com, 33359, None)
2018-08-08 12:37:39,772 [dispatcher-event-loop-5] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:37714 with 11.1 GB RAM, BlockManagerId(13, lpwhdqdnp08.npd.com, 37714, None)
2018-08-08 12:37:39,885 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:47282) with ID 10
2018-08-08 12:37:39,909 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:51940) with ID 17
2018-08-08 12:37:39,937 [dispatcher-event-loop-20] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:38880 with 11.1 GB RAM, BlockManagerId(10, lpwhdqdnp10.npd.com, 38880, None)
2018-08-08 12:37:39,968 [dispatcher-event-loop-24] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:43876 with 11.1 GB RAM, BlockManagerId(17, lpwhdqdnp01.npd.com, 43876, None)
2018-08-08 12:37:41,063 [dispatcher-event-loop-8] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:54368) with ID 24
2018-08-08 12:37:41,103 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:47448) with ID 22
2018-08-08 12:37:41,111 [dispatcher-event-loop-34] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:32936 with 11.1 GB RAM, BlockManagerId(24, lpwhdqdnp07.npd.com, 32936, None)
2018-08-08 12:37:41,132 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:36992) with ID 21
2018-08-08 12:37:41,141 [dispatcher-event-loop-21] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:38472 with 11.1 GB RAM, BlockManagerId(22, lpwhdqdnp03.npd.com, 38472, None)
2018-08-08 12:37:41,165 [dispatcher-event-loop-33] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:46686) with ID 26
2018-08-08 12:37:41,171 [dispatcher-event-loop-12] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:43513 with 11.1 GB RAM, BlockManagerId(21, lpwhdqdnp02.npd.com, 43513, None)
2018-08-08 12:37:41,174 [dispatcher-event-loop-22] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:40208) with ID 19
2018-08-08 12:37:41,203 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:40206) with ID 29
2018-08-08 12:37:41,205 [dispatcher-event-loop-19] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:34914) with ID 28
2018-08-08 12:37:41,215 [dispatcher-event-loop-0] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:36677 with 11.1 GB RAM, BlockManagerId(26, lpwhdqdnp06.npd.com, 36677, None)
2018-08-08 12:37:41,220 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:47290) with ID 20
2018-08-08 12:37:41,220 [dispatcher-event-loop-39] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:42060 with 11.1 GB RAM, BlockManagerId(19, lpwhdqdnp09.npd.com, 42060, None)
2018-08-08 12:37:41,228 [dispatcher-event-loop-35] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:35822) with ID 25
2018-08-08 12:37:41,240 [dispatcher-event-loop-13] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53416) with ID 23
2018-08-08 12:37:41,246 [dispatcher-event-loop-17] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:44742 with 11.1 GB RAM, BlockManagerId(29, lpwhdqdnp09.npd.com, 44742, None)
2018-08-08 12:37:41,247 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:47292) with ID 30
2018-08-08 12:37:41,248 [dispatcher-event-loop-21] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:37256 with 11.1 GB RAM, BlockManagerId(28, lpwhdqdnp05.npd.com, 37256, None)
2018-08-08 12:37:41,273 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:44655 with 11.1 GB RAM, BlockManagerId(20, lpwhdqdnp10.npd.com, 44655, None)
2018-08-08 12:37:41,281 [dispatcher-event-loop-24] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:42748 with 11.1 GB RAM, BlockManagerId(25, lpwhdqdnp04.npd.com, 42748, None)
2018-08-08 12:37:41,285 [dispatcher-event-loop-18] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:41538 with 11.1 GB RAM, BlockManagerId(23, lpwhdqdnp08.npd.com, 41538, None)
2018-08-08 12:37:41,295 [dispatcher-event-loop-25] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:42103 with 11.1 GB RAM, BlockManagerId(30, lpwhdqdnp10.npd.com, 42103, None)
2018-08-08 12:37:41,333 [dispatcher-event-loop-15] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:51948) with ID 27
2018-08-08 12:37:41,385 [dispatcher-event-loop-29] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:42567 with 11.1 GB RAM, BlockManagerId(27, lpwhdqdnp01.npd.com, 42567, None)
2018-08-08 12:37:44,267 [dispatcher-event-loop-39] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:47304) with ID 40
2018-08-08 12:37:44,290 [dispatcher-event-loop-12] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:54378) with ID 44
2018-08-08 12:37:44,312 [dispatcher-event-loop-35] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:46696) with ID 56
2018-08-08 12:37:44,316 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:40286) with ID 49
2018-08-08 12:37:44,317 [dispatcher-event-loop-5] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:39059 with 11.1 GB RAM, BlockManagerId(40, lpwhdqdnp10.npd.com, 39059, None)
2018-08-08 12:37:44,319 [dispatcher-event-loop-17] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:35832) with ID 35
2018-08-08 12:37:44,331 [dispatcher-event-loop-38] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:40282) with ID 59
2018-08-08 12:37:44,335 [dispatcher-event-loop-18] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:40284) with ID 39
2018-08-08 12:37:44,336 [dispatcher-event-loop-18] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:47306) with ID 60
2018-08-08 12:37:44,345 [dispatcher-event-loop-39] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:38526 with 11.1 GB RAM, BlockManagerId(44, lpwhdqdnp07.npd.com, 38526, None)
2018-08-08 12:37:44,347 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:34924) with ID 38
2018-08-08 12:37:44,348 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:34926) with ID 58
2018-08-08 12:37:44,349 [dispatcher-event-loop-12] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:54380) with ID 33
2018-08-08 12:37:44,356 [dispatcher-event-loop-19] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:54384) with ID 54
2018-08-08 12:37:44,369 [dispatcher-event-loop-26] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:44168 with 11.1 GB RAM, BlockManagerId(49, lpwhdqdnp09.npd.com, 44168, None)
2018-08-08 12:37:44,373 [dispatcher-event-loop-10] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:33824 with 11.1 GB RAM, BlockManagerId(56, lpwhdqdnp06.npd.com, 33824, None)
2018-08-08 12:37:44,374 [dispatcher-event-loop-36] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:47460) with ID 32
2018-08-08 12:37:44,374 [dispatcher-event-loop-10] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:36668 with 11.1 GB RAM, BlockManagerId(35, lpwhdqdnp04.npd.com, 36668, None)
2018-08-08 12:37:44,379 [dispatcher-event-loop-5] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:46295 with 11.1 GB RAM, BlockManagerId(59, lpwhdqdnp09.npd.com, 46295, None)
2018-08-08 12:37:44,381 [dispatcher-event-loop-21] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:45589 with 11.1 GB RAM, BlockManagerId(39, lpwhdqdnp09.npd.com, 45589, None)
2018-08-08 12:37:44,383 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:38242 with 11.1 GB RAM, BlockManagerId(60, lpwhdqdnp10.npd.com, 38242, None)
2018-08-08 12:37:44,385 [dispatcher-event-loop-33] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:47308) with ID 50
2018-08-08 12:37:44,388 [dispatcher-event-loop-28] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:46698) with ID 36
2018-08-08 12:37:44,397 [dispatcher-event-loop-25] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:34200 with 11.1 GB RAM, BlockManagerId(58, lpwhdqdnp05.npd.com, 34200, None)
2018-08-08 12:37:44,397 [dispatcher-event-loop-25] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:46395 with 11.1 GB RAM, BlockManagerId(33, lpwhdqdnp07.npd.com, 46395, None)
2018-08-08 12:37:44,401 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:35836) with ID 55
2018-08-08 12:37:44,403 [dispatcher-event-loop-15] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:34899 with 11.1 GB RAM, BlockManagerId(38, lpwhdqdnp05.npd.com, 34899, None)
2018-08-08 12:37:44,406 [dispatcher-event-loop-37] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:35834) with ID 45
2018-08-08 12:37:44,411 [dispatcher-event-loop-32] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:35141 with 11.1 GB RAM, BlockManagerId(54, lpwhdqdnp07.npd.com, 35141, None)
2018-08-08 12:37:44,414 [dispatcher-event-loop-19] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:41976 with 11.1 GB RAM, BlockManagerId(32, lpwhdqdnp03.npd.com, 41976, None)
2018-08-08 12:37:44,422 [dispatcher-event-loop-14] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:34928) with ID 48
2018-08-08 12:37:44,423 [dispatcher-event-loop-14] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53428) with ID 43
2018-08-08 12:37:44,428 [dispatcher-event-loop-36] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:41031 with 11.1 GB RAM, BlockManagerId(50, lpwhdqdnp10.npd.com, 41031, None)
2018-08-08 12:37:44,430 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2018-08-08 12:37:44,437 [dispatcher-event-loop-21] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:40491 with 11.1 GB RAM, BlockManagerId(36, lpwhdqdnp06.npd.com, 40491, None)
2018-08-08 12:37:44,443 [dispatcher-event-loop-38] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:46700) with ID 46
2018-08-08 12:37:44,448 [dispatcher-event-loop-29] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:45632 with 11.1 GB RAM, BlockManagerId(55, lpwhdqdnp04.npd.com, 45632, None)
2018-08-08 12:37:44,449 [dispatcher-event-loop-25] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:37004) with ID 41
2018-08-08 12:37:44,456 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:47462) with ID 42
2018-08-08 12:37:44,461 [dispatcher-event-loop-16] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:47464) with ID 52
2018-08-08 12:37:44,468 [dispatcher-event-loop-32] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:42710 with 11.1 GB RAM, BlockManagerId(45, lpwhdqdnp04.npd.com, 42710, None)
2018-08-08 12:37:44,470 [dispatcher-event-loop-12] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:44365 with 11.1 GB RAM, BlockManagerId(43, lpwhdqdnp08.npd.com, 44365, None)
2018-08-08 12:37:44,475 [dispatcher-event-loop-0] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:33164 with 11.1 GB RAM, BlockManagerId(48, lpwhdqdnp05.npd.com, 33164, None)
2018-08-08 12:37:44,483 [dispatcher-event-loop-26] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:51958) with ID 37
2018-08-08 12:37:44,496 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:51962) with ID 47
2018-08-08 12:37:44,497 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53432) with ID 34
2018-08-08 12:37:44,499 [dispatcher-event-loop-13] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:40794 with 11.1 GB RAM, BlockManagerId(46, lpwhdqdnp06.npd.com, 40794, None)
2018-08-08 12:37:44,499 [dispatcher-event-loop-13] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:45274 with 11.1 GB RAM, BlockManagerId(42, lpwhdqdnp03.npd.com, 45274, None)
2018-08-08 12:37:44,502 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:37006) with ID 51
2018-08-08 12:37:44,503 [dispatcher-event-loop-1] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:38286 with 11.1 GB RAM, BlockManagerId(52, lpwhdqdnp03.npd.com, 38286, None)
2018-08-08 12:37:44,503 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:37008) with ID 31
2018-08-08 12:37:44,508 [dispatcher-event-loop-30] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53430) with ID 53
2018-08-08 12:37:44,509 [dispatcher-event-loop-25] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:40112 with 11.1 GB RAM, BlockManagerId(41, lpwhdqdnp02.npd.com, 40112, None)
2018-08-08 12:37:44,529 [dispatcher-event-loop-15] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:51960) with ID 57
2018-08-08 12:37:44,542 [dispatcher-event-loop-16] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:42780 with 11.1 GB RAM, BlockManagerId(37, lpwhdqdnp01.npd.com, 42780, None)
2018-08-08 12:37:44,546 [dispatcher-event-loop-20] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:35661 with 11.1 GB RAM, BlockManagerId(34, lpwhdqdnp08.npd.com, 35661, None)
2018-08-08 12:37:44,548 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - loading hive config file: file:/etc/spark2/2.6.3.0-235/0/hive-site.xml
2018-08-08 12:37:44,550 [dispatcher-event-loop-37] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:46382 with 11.1 GB RAM, BlockManagerId(47, lpwhdqdnp01.npd.com, 46382, None)
2018-08-08 12:37:44,552 [dispatcher-event-loop-2] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:37430 with 11.1 GB RAM, BlockManagerId(51, lpwhdqdnp02.npd.com, 37430, None)
2018-08-08 12:37:44,553 [dispatcher-event-loop-32] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:33448 with 11.1 GB RAM, BlockManagerId(31, lpwhdqdnp02.npd.com, 33448, None)
2018-08-08 12:37:44,565 [dispatcher-event-loop-12] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:42881 with 11.1 GB RAM, BlockManagerId(53, lpwhdqdnp08.npd.com, 42881, None)
2018-08-08 12:37:44,580 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/').
2018-08-08 12:37:44,581 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/'.
2018-08-08 12:37:44,588 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a47e709{/SQL,null,AVAILABLE,@Spark}
2018-08-08 12:37:44,589 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a0a504{/SQL/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:44,589 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d9583d1{/SQL/execution,null,AVAILABLE,@Spark}
2018-08-08 12:37:44,590 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f236f4{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-08-08 12:37:44,591 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c7b1d8b{/static/sql,null,AVAILABLE,@Spark}
2018-08-08 12:37:44,594 [dispatcher-event-loop-6] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:37502 with 11.1 GB RAM, BlockManagerId(57, lpwhdqdnp01.npd.com, 37502, None)
2018-08-08 12:37:44,973 [Thread-3] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-08-08 12:37:45,428 [Thread-3] INFO  hive.metastore - Trying to connect to metastore with URI thrift://lpwhdqnnp01.npd.com:9083
2018-08-08 12:37:45,457 [Thread-3] INFO  hive.metastore - Connected to metastore.
2018-08-08 12:37:45,605 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/47ad40f4-1d22-46f3-826e-dbfef6326234_resources
2018-08-08 12:37:45,608 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/47ad40f4-1d22-46f3-826e-dbfef6326234
2018-08-08 12:37:45,610 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/47ad40f4-1d22-46f3-826e-dbfef6326234
2018-08-08 12:37:45,612 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/47ad40f4-1d22-46f3-826e-dbfef6326234/_tmp_space.db
2018-08-08 12:37:45,614 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-08-08 12:37:45,786 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/a73a9ae8-05cf-49d9-ad41-6a054efd8fc9_resources
2018-08-08 12:37:45,788 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/a73a9ae8-05cf-49d9-ad41-6a054efd8fc9
2018-08-08 12:37:45,789 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/a73a9ae8-05cf-49d9-ad41-6a054efd8fc9
2018-08-08 12:37:45,792 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/a73a9ae8-05cf-49d9-ad41-6a054efd8fc9/_tmp_space.db
2018-08-08 12:37:45,793 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-08-08 12:37:45,831 [Thread-3] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
mysql
hivedb
['dictionary_frontend', 'PACEDICTIONARY']
['dqdictionaryhivedb']
['REFS']
['DICTIONARY_STATUS', 'POIID_DETAILS', 'TIMETRACKER']
['uniqueodsposoutlet2_int', 'uniqueodsitems2_int', 'uniqueodsposoutlet_int_limit_bs', 'uniqueodsitems_int_limit_bs']
set([160, 161, 162, 174])
{'historysearch': 1, 'writebackresults': 1, 'updatehivetable': 1, 'updateodsposoutlettable': 1, 'creatingmap': 0, 'newadditempoiddetection': 1, 'loadodsitem': 1, 'loadposoutlet': 1, 'updateodsitemtable': 1, 'newaddmapping': 1, 'partitioncreator': 0, 'forward_tables': 0, 'upcskusearch': 1, 'transfer_data': 0}
configuration class
2018-08-08 12:37:45,905 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 166 this is the begining of application
2018-08-08 12:37:45,905 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 167 adding all the project directories
2018-08-08 12:37:45,905 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 171 stages are: ['historysearch', 'writebackresults', 'updatehivetable', 'updateodsposoutlettable', 'creatingmap', 'newadditempoiddetection', 'loadodsitem', 'loadposoutlet', 'updateodsitemtable', 'newaddmapping', 'partitioncreator', 'forward_tables', 'upcskusearch', 'transfer_data']
Usage: snapshotdetector number of days0
08-07-2018 14:48:56
start reading snapshots for posoutlet table
so total dates: ['201883']
Get the filtered files to read
file list len: 3 part-m-00292.bz2
start reading snapshot files
List of files gonna be loaded: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00292.bz2,hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00290.bz2,hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00289.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00292.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00290.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00290.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00289.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00289.bz2
snapshot data count before businesswise filtering: 2315064
upload itemIdWithPartition08.txt file
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/itemidMapper/itemIdWithPartition08.txt
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/partitionList/partition_list.txt
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/fileSource/partition_list.txt hdfs:////npd/s_test2/partitionList/
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/maps/itemIdWithPartition08.txt hdfs:////npd/s_test2/itemidMapper/
0 and  and 
Reading itemId meta data
getting the last two columns
creating a temptable
Getting distinct partition names
Performing the join operation
snapshot data count: 244437
snapshot history data count: 243863
getting the list of files
restructuring the files
End of file processing.
End of the function
Start loading the base dictionary
Executing command: select poi_id,business_id,posoutlet,outletdivision,outletdepartment,outletsubdepartment,outletclass,outletsubclass,outletbrand,outletitemnumber,outletdescription,outletbrandmatch,outletitemnumbermatch,outletdescriptionmatch,sku,manufacturercodetype,manufacturercode,zzzppmonthfrom,zzzppmonthto,zzzppmonthlastused,itemid,itemtype,price,manufacturercodestatus,loadid,status,added,updated,ppweekfrom,ppweekto,ppweeklastused,matched_country_code,previous_poiid,include_data_ppmonthfrom,include_data_ppweekfrom,manufacturercodematch,skumatch,unitofmeasure,packsize,manufacturername,manufacturernamematch,privatelabel,outletdescriptionsupplement,total_confidence_score,parent_poiid,parent_poiid_status, partitioner  from dqdictionaryhivedb.uniqueodsposoutlet_int_limit_bs
size of the loaded base dictionary is: 21667790
Usage: snapshotdetector number of days0
08-07-2018 14:44:53
start reading snapshots for odsitem table
so total dates: ['201883']
Get the filtered files to read
file list len: 3 part-m-00294.bz2
start reading snapshot files
List of files gonna be loaded: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00294.bz2,hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00292.bz2,hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00291.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00294.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00292.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00292.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00291.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00291.bz2
snapshot data count before businesswise filtering: 3726407
snapshot data count: 119369
snapshot history data count: 119369
getting the list of files
Start loading the base dictionary
Executing command: select itemid, businessid, subcategoryn, itemnumber, unitspackage,fld01, fld02, fld03, fld04, fld05, fld06, fld07, fld08, fld09, fld10, fld11, fld12, fld13, fld14, fld15, fld16, fld17, fld18, fld19, fld20, fld21, fld22, fld23, fld24, fld25, fld26, fld27, fld28, fld29, fld30, fld31, fld32, fld33, fld34, fld35, fld36, fld37, fld38, fld39, fld40, fld41, fld42, fld43, fld44, fld45, fld46, fld47, fld48, fld49, fld50, fld51, fld52, fld53, fld54, fld55, fld56, fld57, fld58, fld59, fld60, fld61, fld62, fld63, fld64, fld65, fld66, fld67, fld68, fld69, fld70, fld71, fld72, fld73, fld74, fld75, fld76, fld77, fld78, fld79, fld80, fld81, fld82, fld83, fld84, fld85, fld86, fld87, fld88, fld89, fld90, fld91, fld92, fld93, fld94, fld95, fld96, fld97, fld98, fld99, status, added, updated, vfld01, vfld02, vfld03, vfld04, vfld05, country_code, groupitemid, parentitemid, parentitemid_status, outletitem_map_change_date, lockdown_status from dqdictionaryhivedb.uniqueodsitems_int_limit_bs
size of the loaded base dictionary is: 1490634
new add itemids count : 244
filtered poiid count 288
Traceback (most recent call last):
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/main.py", line 87, in <module>
    mainOb.executor()
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/main.py", line 75, in executor
    mainOps()
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/dictionary/mainKnitter.py", line 220, in mainOps
    itemid_frame,poiid_frame,remap_item, remap_poiid = getNewadds(rddOds = rddOdsitem, rddOutlet = rddOutlet, config_ = configOb, sqlContext = sqlContext, gv = gv) if newaddversion is 1 and configOb.stage['newadditempoiddetection'] is not 0 else getNewaddsv2(snapshotItemid = rddOdsitem, snapshotPoiid = rddOutlet, dictPositems = baseRddodsitem, dictPosoutlet = baseRddoutlet, spark_context = gv.spark_context, configOb = configOb, sqlc = sqlc ) if configOb.stage['newadditempoiddetection'] is not 0 else (None, None, None, None)
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/dictionary/findNewAdd/find_new_itemid.py", line 408, in getNewaddsv2
    rRemapPoiid = unionAlldf(reampPoiidtups, sqlc = sqlc, sc_ = spark_context )
NameError: global name 'reampPoiidtups' is not defined
Ivy Default Cache set to: /home/abdul.sabbir/.ivy2/cache
The jars for the packages stored in: /home/abdul.sabbir/.ivy2/jars
:: loading settings :: url = jar:file:/usr/hdp/2.6.3.0-235/spark2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
mysql#mysql-connector-java added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found mysql#mysql-connector-java;5.1.39 in central
:: resolution report :: resolve 167ms :: artifacts dl 6ms
	:: modules in use:
	mysql#mysql-connector-java;5.1.39 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/5ms)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:ERROR Could not instantiate class [INFO].
java.lang.ClassNotFoundException: INFO
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Could not instantiate class [WARNING].
java.lang.ClassNotFoundException: WARNING
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:WARN No such property [datePattern] in org.apache.log4j.FileAppender.
log4j:ERROR Could not instantiate class [DEBUG].
java.lang.ClassNotFoundException: DEBUG
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
main function for this pyspark project is going to be initiated soon .....
current working directory is :/hdpdata/pysparkProject/dqMachine
Operating system name : Linux
Machine Name : lpwhdqdnp01.npd.com
operating system version : 3.10.0-693.5.2.el7.x86_64
last access time #1 SMP Fri Oct 13 10:46:25 EDT 2017
operating system architecture : x86_64
Usage: snapshot detector number of days: 0
2018-09-06 16:58:41,839 [Thread-3] INFO  org.apache.spark.SparkContext - Running Spark version 2.2.0.2.6.3.0-235
2018-09-06 16:58:42,088 [Thread-3] WARN  org.apache.spark.SparkConf - In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
2018-09-06 16:58:42,097 [Thread-3] INFO  org.apache.spark.SparkContext - Submitted application: dqMachineApplication
2018-09-06 16:58:42,179 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-09-06 16:58:42,180 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-09-06 16:58:42,180 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-09-06 16:58:42,181 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-09-06 16:58:42,181 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-09-06 16:58:42,429 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 39261.
2018-09-06 16:58:42,445 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2018-09-06 16:58:42,461 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2018-09-06 16:58:42,463 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-09-06 16:58:42,464 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2018-09-06 16:58:42,471 [Thread-3] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /hdpdata/tmp/blockmgr-5dfaa3ad-38af-4b2a-9bab-b3b731072cdf
2018-09-06 16:58:42,491 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 5.2 GB
2018-09-06 16:58:42,619 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2018-09-06 16:58:42,689 [Thread-3] INFO  org.spark_project.jetty.util.log - Logging initialized @2508ms
2018-09-06 16:58:42,744 [Thread-3] INFO  org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
2018-09-06 16:58:42,757 [Thread-3] INFO  org.spark_project.jetty.server.Server - Started @2577ms
2018-09-06 16:58:42,769 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2018-09-06 16:58:42,775 [Thread-3] INFO  org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@74b5afa8{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2018-09-06 16:58:42,775 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
2018-09-06 16:58:42,797 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53879ffe{/jobs,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,798 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7db39e6d{/jobs/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,799 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f741493{/jobs/job,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,800 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d883e99{/jobs/job/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,801 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@449a95e9{/stages,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,801 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12d4f2d7{/stages/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,802 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b860e25{/stages/stage,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,803 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cb33125{/stages/stage/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,804 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@71fbb06f{/stages/pool,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,805 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c05bd1e{/stages/pool/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,806 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76c7d49a{/storage,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,806 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6a747ab4{/storage/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,807 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63f67ae7{/storage/rdd,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,808 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27b7e7ef{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,808 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3418092b{/environment,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,809 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ecf9888{/environment/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,809 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@659c3a71{/executors,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,810 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d89674a{/executors/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,811 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@21e22179{/executors/threadDump,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,812 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2d025e89{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,817 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@76d99a56{/static,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,818 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53083917{/,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,819 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3bd2e58d{/api,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,819 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53881da6{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,820 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c829cba{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-09-06 16:58:42,822 [Thread-3] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.231.1.14:4041
2018-09-06 16:58:42,892 [Thread-3] WARN  org.apache.spark.scheduler.FairSchedulableBuilder - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
2018-09-06 16:58:42,894 [Thread-3] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
2018-09-06 16:58:43,617 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Looking for the active RM in [rm1, rm2]...
2018-09-06 16:58:43,667 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Found active RM [rm1]
2018-09-06 16:58:43,668 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Requesting a new application from cluster with 10 NodeManagers
2018-09-06 16:58:43,706 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Verifying our application has not requested more than the maximum memory capability of the cluster (196608 MB per container)
2018-09-06 16:58:43,706 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Will allocate AM container, with 896 MB memory including 384 MB overhead
2018-09-06 16:58:43,707 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up container launch context for our AM
2018-09-06 16:58:43,711 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up the launch environment for our AM container
2018-09-06 16:58:43,719 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Preparing resources for our AM container
2018-09-06 16:58:44,578 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-09-06 16:58:44,581 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Source and destination file systems are the same. Not copying hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-09-06 16:58:44,640 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16831/mysql_mysql-connector-java-5.1.39.jar
2018-09-06 16:58:44,822 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16831/pyspark.zip
2018-09-06 16:58:44,848 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16831/py4j-0.10.4-src.zip
2018-09-06 16:58:44,870 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/pysparkProject/dqMachine/dq_machine.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16831/dq_machine.zip
2018-09-06 16:58:44,893 [Thread-3] WARN  org.apache.spark.deploy.yarn.Client - Same path resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.
2018-09-06 16:58:44,917 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/tmp/spark-0fd60774-8a78-4b1a-8437-bd513614ef05/__spark_conf__784400774020494769.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16831/__spark_conf__.zip
2018-09-06 16:58:44,954 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-09-06 16:58:44,955 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-09-06 16:58:44,955 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-09-06 16:58:44,955 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-09-06 16:58:44,955 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-09-06 16:58:44,960 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Submitting application application_1524081153081_16831 to ResourceManager
2018-09-06 16:58:44,991 [Thread-3] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1524081153081_16831
2018-09-06 16:58:44,993 [Thread-3] INFO  org.apache.spark.scheduler.cluster.SchedulerExtensionServices - Starting Yarn extension services with app application_1524081153081_16831 and attemptId None
2018-09-06 16:58:45,999 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16831 (state: ACCEPTED)
2018-09-06 16:58:46,003 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1536267524970
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16831/
	 user: abdul.sabbir
2018-09-06 16:58:47,005 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16831 (state: ACCEPTED)
2018-09-06 16:58:47,969 [dispatcher-event-loop-38] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lpwhdqnnp01.npd.com,lpwhdqnnp02.npd.com, PROXY_URI_BASES -> http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16831,http://lpwhdqnnp02.npd.com:8088/proxy/application_1524081153081_16831), /proxy/application_1524081153081_16831
2018-09-06 16:58:47,970 [dispatcher-event-loop-38] INFO  org.apache.spark.ui.JettyUtils - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2018-09-06 16:58:48,007 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16831 (state: ACCEPTED)
2018-09-06 16:58:48,273 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
2018-09-06 16:58:49,009 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16831 (state: RUNNING)
2018-09-06 16:58:49,010 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.231.1.22
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1536267524970
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16831/
	 user: abdul.sabbir
2018-09-06 16:58:49,011 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Application application_1524081153081_16831 has started running.
2018-09-06 16:58:49,020 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35803.
2018-09-06 16:58:49,020 [Thread-3] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.231.1.14:35803
2018-09-06 16:58:49,023 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-09-06 16:58:49,025 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.231.1.14, 35803, None)
2018-09-06 16:58:49,028 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.231.1.14:35803 with 5.2 GB RAM, BlockManagerId(driver, 10.231.1.14, 35803, None)
2018-09-06 16:58:49,032 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.231.1.14, 35803, None)
2018-09-06 16:58:49,032 [Thread-3] INFO  org.apache.spark.storage.BlockManager - external shuffle service port = 7447
2018-09-06 16:58:49,032 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.231.1.14, 35803, None)
2018-09-06 16:58:49,202 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@192293dc{/metrics/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:49,355 [Thread-3] INFO  org.apache.spark.scheduler.EventLoggingListener - Logging events to file:/hdpdata/logs/application_1524081153081_16831
2018-09-06 16:58:51,450 [dispatcher-event-loop-38] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:53602) with ID 6
2018-09-06 16:58:51,487 [dispatcher-event-loop-36] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:40104) with ID 5
2018-09-06 16:58:51,506 [dispatcher-event-loop-19] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:41582 with 4.1 GB RAM, BlockManagerId(6, lpwhdqdnp07.npd.com, 41582, None)
2018-09-06 16:58:51,538 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:38594 with 4.1 GB RAM, BlockManagerId(5, lpwhdqdnp10.npd.com, 38594, None)
2018-09-06 16:58:51,549 [dispatcher-event-loop-18] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:57516) with ID 4
2018-09-06 16:58:51,571 [dispatcher-event-loop-22] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:53360) with ID 3
2018-09-06 16:58:51,591 [dispatcher-event-loop-22] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:49864) with ID 1
2018-09-06 16:58:51,597 [dispatcher-event-loop-16] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:45920 with 4.1 GB RAM, BlockManagerId(4, lpwhdqdnp04.npd.com, 45920, None)
2018-09-06 16:58:51,612 [dispatcher-event-loop-22] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:34458) with ID 2
2018-09-06 16:58:51,623 [dispatcher-event-loop-24] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:39148 with 4.1 GB RAM, BlockManagerId(3, lpwhdqdnp01.npd.com, 39148, None)
2018-09-06 16:58:51,636 [dispatcher-event-loop-37] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:42322 with 4.1 GB RAM, BlockManagerId(1, lpwhdqdnp08.npd.com, 42322, None)
2018-09-06 16:58:51,657 [dispatcher-event-loop-35] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:38938 with 4.1 GB RAM, BlockManagerId(2, lpwhdqdnp02.npd.com, 38938, None)
2018-09-06 16:58:51,799 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:43220) with ID 7
2018-09-06 16:58:51,840 [dispatcher-event-loop-17] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:35442 with 4.1 GB RAM, BlockManagerId(7, lpwhdqdnp09.npd.com, 35442, None)
2018-09-06 16:58:52,147 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:57486) with ID 10
2018-09-06 16:58:52,192 [dispatcher-event-loop-10] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:40033 with 4.1 GB RAM, BlockManagerId(10, lpwhdqdnp03.npd.com, 40033, None)
2018-09-06 16:58:52,232 [dispatcher-event-loop-23] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:36354) with ID 9
2018-09-06 16:58:52,252 [dispatcher-event-loop-23] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:49918) with ID 8
2018-09-06 16:58:52,252 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2018-09-06 16:58:52,286 [dispatcher-event-loop-14] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:36065 with 4.1 GB RAM, BlockManagerId(9, lpwhdqdnp06.npd.com, 36065, None)
2018-09-06 16:58:52,300 [dispatcher-event-loop-25] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:35519 with 4.1 GB RAM, BlockManagerId(8, lpwhdqdnp05.npd.com, 35519, None)
2018-09-06 16:58:52,352 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - loading hive config file: file:/etc/spark2/2.6.3.0-235/0/hive-site.xml
2018-09-06 16:58:52,373 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/').
2018-09-06 16:58:52,373 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/'.
2018-09-06 16:58:52,378 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7097ff8d{/SQL,null,AVAILABLE,@Spark}
2018-09-06 16:58:52,378 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@414fad7a{/SQL/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:52,379 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f186012{/SQL/execution,null,AVAILABLE,@Spark}
2018-09-06 16:58:52,379 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3841b6e5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-09-06 16:58:52,380 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d91a5d0{/static/sql,null,AVAILABLE,@Spark}
2018-09-06 16:58:52,743 [Thread-3] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-09-06 16:58:53,184 [Thread-3] INFO  hive.metastore - Trying to connect to metastore with URI thrift://lpwhdqnnp01.npd.com:9083
2018-09-06 16:58:53,215 [Thread-3] INFO  hive.metastore - Connected to metastore.
2018-09-06 16:58:53,512 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/52efdf24-0bb6-45a5-b90a-e1af7b1f50fe_resources
2018-09-06 16:58:53,516 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/52efdf24-0bb6-45a5-b90a-e1af7b1f50fe
2018-09-06 16:58:53,518 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/52efdf24-0bb6-45a5-b90a-e1af7b1f50fe
2018-09-06 16:58:53,521 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/52efdf24-0bb6-45a5-b90a-e1af7b1f50fe/_tmp_space.db
2018-09-06 16:58:53,523 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-09-06 16:58:53,685 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/5348d9e2-c8ec-46bc-ab2d-812ed47be3e3_resources
2018-09-06 16:58:53,688 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/5348d9e2-c8ec-46bc-ab2d-812ed47be3e3
2018-09-06 16:58:53,689 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/5348d9e2-c8ec-46bc-ab2d-812ed47be3e3
2018-09-06 16:58:53,692 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/5348d9e2-c8ec-46bc-ab2d-812ed47be3e3/_tmp_space.db
2018-09-06 16:58:53,693 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-09-06 16:58:53,728 [Thread-3] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
mysql
hivedb
['dictionary_frontend', 'PACEDICTIONARY']
['dqdictionaryhivedb']
['REFS']
['DICTIONARY_STATUS', 'POIID_DETAILS', 'TIMETRACKER']
['uniqueodsposoutlet2_int', 'uniqueodsitems2_int', 'uniqueodsposoutlet_int_limit_bs', 'uniqueodsitems_int_limit_bs']
set([160, 161, 162, 174])
{'historysearch': 1, 'writebackresults': 1, 'updatehivetable': 1, 'write_to_hdfs': 0, 'creatingmap': 1, 'newadditempoiddetection': 1, 'generate_map_on_latest_data': 0, 'loadodsitem': 1, 'transfer_data_to_hdfs': 1, 'loadposoutlet': 1, 'updateodsitemtable': 1, 'newaddmapping': 1, 'partitioncreator': 0, 'forward_tables': 0, 'updateposoutlettable': 1, 'upcskusearch': 1}
configuration class
2018-09-06 16:58:53,832 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 166 this is the begining of application
2018-09-06 16:58:53,832 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 167 adding all the project directories
2018-09-06 16:58:53,833 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 171 stages are: ['historysearch', 'writebackresults', 'updatehivetable', 'write_to_hdfs', 'creatingmap', 'newadditempoiddetection', 'generate_map_on_latest_data', 'loadodsitem', 'transfer_data_to_hdfs', 'loadposoutlet', 'updateodsitemtable', 'newaddmapping', 'partitioncreator', 'forward_tables', 'updateposoutlettable', 'upcskusearch']
Usage: snapshotdetector number of days0
recalculated  number of partition 15.0
15
09-06-2018 16:12:52
['hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00392.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00390.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00389.bz2']
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00392.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00390.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00390.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00389.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00389.bz2
snapshot data count before businesswise filtering: 944373
recalculated  number of partition 1.0
upload itemIdWithPartition08.txt file
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/itemidMapper/itemIdWithPartition08.txt
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/partitionList/partition_list.txt
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/fileSource/partition_list.txt hdfs:////npd/s_test2/partitionList/
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/maps/itemIdWithPartition08.txt hdfs:////npd/s_test2/itemidMapper/
0 and  and 
Reading itemId meta data
getting the last two columns
creating a temptable
Getting distinct partition names
Performing the join operation
snapshot data count: 4260
2018-09-06 16:59:40,091 [Yarn application state monitor] ERROR org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Yarn application has already exited with state KILLED!
2018-09-06 16:59:40,161 [rpc-server-3-3] ERROR org.apache.spark.network.client.TransportResponseHandler - Still have 1 requests outstanding when connection from /10.231.1.22:43194 is closed
2018-09-06 16:59:40,164 [rpc-server-3-3] ERROR org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint - Sending RequestExecutors(0,0,Map(),Set()) to AM was unsuccessful
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
2018-09-06 16:59:40,169 [Yarn application state monitor] ERROR org.apache.spark.util.Utils - Uncaught exception in thread Yarn application state monitor
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:551)
	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:94)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:151)
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:517)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1670)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1927)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
Traceback (most recent call last):
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/main.py", line 90, in <module>
    mainOb.executor()
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/main.py", line 78, in executor
    mainOps()
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/dictionary/mainKnitter.py", line 223, in mainOps
    rddOutlet, baseRddoutlet, fileListPosoutlet, listOffiles, businessSet, partitionSet, itemidRdd, rddOutletwithPartition = loadingData(data_path =  configOb.input_path[2], baseDir = configOb.input_path[0] , spark = gv.spark_context, fileType = 0, ranges = int(sys.argv[1]), odsitem = 0, businessList = configOb.businessSets, readHive = configOb.read_hive_odspos, configOb = configOb) if configOb.stage['loadposoutlet'] is not 0 else (None, None, [], None, None, None, None, None)
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/dictionary/writeBackModule/readAndload.py", line 142, in loadingData
    print "snapshot history data count: " + str(snapshotRddwithPartition.count())
  File "/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 427, in count
  File "/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o1802.count.
: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)
	at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:88)
	at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:209)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)
	at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:235)
	at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:263)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:235)
	at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:128)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:88)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:218)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:146)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2431)
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)
	at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2838)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2837)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:2430)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job 12 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1927)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:73)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:104)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

Ivy Default Cache set to: /home/abdul.sabbir/.ivy2/cache
The jars for the packages stored in: /home/abdul.sabbir/.ivy2/jars
:: loading settings :: url = jar:file:/usr/hdp/2.6.3.0-235/spark2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
mysql#mysql-connector-java added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found mysql#mysql-connector-java;5.1.39 in central
:: resolution report :: resolve 168ms :: artifacts dl 6ms
	:: modules in use:
	mysql#mysql-connector-java;5.1.39 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/6ms)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:ERROR Could not instantiate class [INFO].
java.lang.ClassNotFoundException: INFO
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Could not instantiate class [WARNING].
java.lang.ClassNotFoundException: WARNING
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:WARN No such property [datePattern] in org.apache.log4j.FileAppender.
log4j:ERROR Could not instantiate class [DEBUG].
java.lang.ClassNotFoundException: DEBUG
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
main function for this pyspark project is going to be initiated soon .....
current working directory is :/hdpdata/pysparkProject/dqMachine
Operating system name : Linux
Machine Name : lpwhdqdnp01.npd.com
operating system version : 3.10.0-693.5.2.el7.x86_64
last access time #1 SMP Fri Oct 13 10:46:25 EDT 2017
operating system architecture : x86_64
Usage: snapshot detector number of days: 0
2018-09-06 16:59:59,789 [Thread-3] INFO  org.apache.spark.SparkContext - Running Spark version 2.2.0.2.6.3.0-235
2018-09-06 17:00:00,038 [Thread-3] WARN  org.apache.spark.SparkConf - In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
2018-09-06 17:00:00,047 [Thread-3] INFO  org.apache.spark.SparkContext - Submitted application: dqMachineApplication
2018-09-06 17:00:00,067 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-09-06 17:00:00,067 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-09-06 17:00:00,068 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-09-06 17:00:00,068 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-09-06 17:00:00,069 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-09-06 17:00:00,392 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 44227.
2018-09-06 17:00:00,408 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2018-09-06 17:00:00,424 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2018-09-06 17:00:00,426 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-09-06 17:00:00,427 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2018-09-06 17:00:00,434 [Thread-3] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /hdpdata/tmp/blockmgr-9fd70d18-cc27-456e-865b-5b83c3f81254
2018-09-06 17:00:00,453 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 5.2 GB
2018-09-06 17:00:00,561 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2018-09-06 17:00:00,628 [Thread-3] INFO  org.spark_project.jetty.util.log - Logging initialized @2474ms
2018-09-06 17:00:00,680 [Thread-3] INFO  org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
2018-09-06 17:00:00,693 [Thread-3] INFO  org.spark_project.jetty.server.Server - Started @2540ms
2018-09-06 17:00:00,706 [Thread-3] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2018-09-06 17:00:00,711 [Thread-3] INFO  org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@21df45de{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2018-09-06 17:00:00,711 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
2018-09-06 17:00:00,733 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@583a2a93{/jobs,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,734 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4d211420{/jobs/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,734 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fc53966{/jobs/job,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,735 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b102e94{/jobs/job/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,736 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@12e76737{/stages,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,737 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ec46760{/stages/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,737 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba3e165{/stages/stage,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,739 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@675ffccd{/stages/stage/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,739 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e5f13e2{/stages/pool,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,740 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@695a9309{/stages/pool/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,741 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a137f7f{/storage,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,741 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4bb8c390{/storage/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,742 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bdd0802{/storage/rdd,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,742 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e5f814a{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,743 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64697603{/environment,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,744 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@265f9b33{/environment/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,744 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11b9383c{/executors,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,745 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f40b3f2{/executors/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,745 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2529d81f{/executors/threadDump,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,746 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b07c994{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,751 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46809216{/static,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,752 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54b7950a{/,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,753 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fb2359f{/api,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,754 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@299b52ec{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,754 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11580de9{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-09-06 17:00:00,756 [Thread-3] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.231.1.14:4041
2018-09-06 17:00:00,817 [Thread-3] WARN  org.apache.spark.scheduler.FairSchedulableBuilder - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
2018-09-06 17:00:00,819 [Thread-3] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
2018-09-06 17:00:01,594 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Looking for the active RM in [rm1, rm2]...
2018-09-06 17:00:01,646 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Found active RM [rm1]
2018-09-06 17:00:01,647 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Requesting a new application from cluster with 10 NodeManagers
2018-09-06 17:00:01,686 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Verifying our application has not requested more than the maximum memory capability of the cluster (196608 MB per container)
2018-09-06 17:00:01,687 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Will allocate AM container, with 896 MB memory including 384 MB overhead
2018-09-06 17:00:01,687 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up container launch context for our AM
2018-09-06 17:00:01,691 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up the launch environment for our AM container
2018-09-06 17:00:01,698 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Preparing resources for our AM container
2018-09-06 17:00:02,571 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-09-06 17:00:02,574 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Source and destination file systems are the same. Not copying hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-09-06 17:00:02,625 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16832/mysql_mysql-connector-java-5.1.39.jar
2018-09-06 17:00:02,804 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16832/pyspark.zip
2018-09-06 17:00:02,827 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16832/py4j-0.10.4-src.zip
2018-09-06 17:00:02,849 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/pysparkProject/dqMachine/dq_machine.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16832/dq_machine.zip
2018-09-06 17:00:02,872 [Thread-3] WARN  org.apache.spark.deploy.yarn.Client - Same path resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.
2018-09-06 17:00:02,896 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/tmp/spark-c660018c-7b27-42d1-9a88-30e3572aadd9/__spark_conf__8848139310118567468.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16832/__spark_conf__.zip
2018-09-06 17:00:02,932 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-09-06 17:00:02,932 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-09-06 17:00:02,932 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-09-06 17:00:02,932 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-09-06 17:00:02,932 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-09-06 17:00:02,937 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Submitting application application_1524081153081_16832 to ResourceManager
2018-09-06 17:00:02,965 [Thread-3] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1524081153081_16832
2018-09-06 17:00:02,967 [Thread-3] INFO  org.apache.spark.scheduler.cluster.SchedulerExtensionServices - Starting Yarn extension services with app application_1524081153081_16832 and attemptId None
2018-09-06 17:00:03,973 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16832 (state: ACCEPTED)
2018-09-06 17:00:03,977 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1536267602947
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16832/
	 user: abdul.sabbir
2018-09-06 17:00:04,978 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16832 (state: ACCEPTED)
2018-09-06 17:00:05,980 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16832 (state: ACCEPTED)
2018-09-06 17:00:06,072 [dispatcher-event-loop-39] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lpwhdqnnp01.npd.com,lpwhdqnnp02.npd.com, PROXY_URI_BASES -> http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16832,http://lpwhdqnnp02.npd.com:8088/proxy/application_1524081153081_16832), /proxy/application_1524081153081_16832
2018-09-06 17:00:06,074 [dispatcher-event-loop-39] INFO  org.apache.spark.ui.JettyUtils - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2018-09-06 17:00:06,584 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
2018-09-06 17:00:06,982 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16832 (state: RUNNING)
2018-09-06 17:00:06,982 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.231.1.15
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1536267602947
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16832/
	 user: abdul.sabbir
2018-09-06 17:00:06,983 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Application application_1524081153081_16832 has started running.
2018-09-06 17:00:06,991 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37003.
2018-09-06 17:00:06,992 [Thread-3] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.231.1.14:37003
2018-09-06 17:00:06,995 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-09-06 17:00:06,996 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.231.1.14, 37003, None)
2018-09-06 17:00:07,000 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.231.1.14:37003 with 5.2 GB RAM, BlockManagerId(driver, 10.231.1.14, 37003, None)
2018-09-06 17:00:07,003 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.231.1.14, 37003, None)
2018-09-06 17:00:07,003 [Thread-3] INFO  org.apache.spark.storage.BlockManager - external shuffle service port = 7447
2018-09-06 17:00:07,004 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.231.1.14, 37003, None)
2018-09-06 17:00:07,157 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@236e925c{/metrics/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:07,288 [Thread-3] INFO  org.apache.spark.scheduler.EventLoggingListener - Logging events to file:/hdpdata/logs/application_1524081153081_16832
2018-09-06 17:00:09,602 [dispatcher-event-loop-39] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:56948) with ID 2
2018-09-06 17:00:09,655 [dispatcher-event-loop-11] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:46432 with 4.1 GB RAM, BlockManagerId(2, lpwhdqdnp02.npd.com, 46432, None)
2018-09-06 17:00:09,733 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:52822) with ID 4
2018-09-06 17:00:09,755 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:53506) with ID 6
2018-09-06 17:00:09,774 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:34680) with ID 1
2018-09-06 17:00:09,783 [dispatcher-event-loop-20] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:43781 with 4.1 GB RAM, BlockManagerId(4, lpwhdqdnp10.npd.com, 43781, None)
2018-09-06 17:00:09,795 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:45750) with ID 7
2018-09-06 17:00:09,806 [dispatcher-event-loop-29] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:34027 with 4.1 GB RAM, BlockManagerId(6, lpwhdqdnp08.npd.com, 34027, None)
2018-09-06 17:00:09,815 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:34578) with ID 3
2018-09-06 17:00:09,821 [dispatcher-event-loop-14] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:37762 with 4.1 GB RAM, BlockManagerId(1, lpwhdqdnp09.npd.com, 37762, None)
2018-09-06 17:00:09,840 [dispatcher-event-loop-30] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:33979 with 4.1 GB RAM, BlockManagerId(7, lpwhdqdnp07.npd.com, 33979, None)
2018-09-06 17:00:09,870 [dispatcher-event-loop-34] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:37442 with 4.1 GB RAM, BlockManagerId(3, lpwhdqdnp04.npd.com, 37442, None)
2018-09-06 17:00:09,891 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.14:59710) with ID 5
2018-09-06 17:00:09,940 [dispatcher-event-loop-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp01.npd.com:39363 with 4.1 GB RAM, BlockManagerId(5, lpwhdqdnp01.npd.com, 39363, None)
2018-09-06 17:00:10,501 [dispatcher-event-loop-13] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:52370) with ID 10
2018-09-06 17:00:10,540 [dispatcher-event-loop-19] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:47734) with ID 9
2018-09-06 17:00:10,543 [dispatcher-event-loop-28] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:38467 with 4.1 GB RAM, BlockManagerId(10, lpwhdqdnp03.npd.com, 38467, None)
2018-09-06 17:00:10,560 [dispatcher-event-loop-19] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:47392) with ID 8
2018-09-06 17:00:10,579 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2018-09-06 17:00:10,583 [dispatcher-event-loop-22] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:46681 with 4.1 GB RAM, BlockManagerId(9, lpwhdqdnp05.npd.com, 46681, None)
2018-09-06 17:00:10,614 [dispatcher-event-loop-33] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:45417 with 4.1 GB RAM, BlockManagerId(8, lpwhdqdnp06.npd.com, 45417, None)
2018-09-06 17:00:10,680 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - loading hive config file: file:/etc/spark2/2.6.3.0-235/0/hive-site.xml
2018-09-06 17:00:10,702 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/').
2018-09-06 17:00:10,703 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/'.
2018-09-06 17:00:10,708 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5cc049c7{/SQL,null,AVAILABLE,@Spark}
2018-09-06 17:00:10,709 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ed3c1{/SQL/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:10,709 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ab5c9c3{/SQL/execution,null,AVAILABLE,@Spark}
2018-09-06 17:00:10,710 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72863938{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-09-06 17:00:10,711 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ecc9e17{/static/sql,null,AVAILABLE,@Spark}
2018-09-06 17:00:11,080 [Thread-3] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-09-06 17:00:11,536 [Thread-3] INFO  hive.metastore - Trying to connect to metastore with URI thrift://lpwhdqnnp01.npd.com:9083
2018-09-06 17:00:11,565 [Thread-3] INFO  hive.metastore - Connected to metastore.
2018-09-06 17:00:11,699 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/f23057a2-933c-4ca1-9cb7-1b413c5cde34_resources
2018-09-06 17:00:11,702 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/f23057a2-933c-4ca1-9cb7-1b413c5cde34
2018-09-06 17:00:11,703 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/f23057a2-933c-4ca1-9cb7-1b413c5cde34
2018-09-06 17:00:11,706 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/f23057a2-933c-4ca1-9cb7-1b413c5cde34/_tmp_space.db
2018-09-06 17:00:11,708 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-09-06 17:00:11,859 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/d06acb56-f07e-4522-9eea-3fcf89c917e6_resources
2018-09-06 17:00:11,861 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/d06acb56-f07e-4522-9eea-3fcf89c917e6
2018-09-06 17:00:11,862 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/d06acb56-f07e-4522-9eea-3fcf89c917e6
2018-09-06 17:00:11,864 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/d06acb56-f07e-4522-9eea-3fcf89c917e6/_tmp_space.db
2018-09-06 17:00:11,865 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-09-06 17:00:11,899 [Thread-3] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
mysql
hivedb
['dictionary_frontend', 'PACEDICTIONARY']
['dqdictionaryhivedb']
['REFS']
['DICTIONARY_STATUS', 'POIID_DETAILS', 'TIMETRACKER']
['uniqueodsposoutlet2_int', 'uniqueodsitems2_int', 'uniqueodsposoutlet_int_limit_bs', 'uniqueodsitems_int_limit_bs']
set([160, 161, 162, 174])
{'historysearch': 1, 'writebackresults': 1, 'updatehivetable': 1, 'write_to_hdfs': 0, 'creatingmap': 1, 'newadditempoiddetection': 1, 'generate_map_on_latest_data': 0, 'loadodsitem': 1, 'transfer_data_to_hdfs': 1, 'loadposoutlet': 1, 'updateodsitemtable': 1, 'newaddmapping': 1, 'partitioncreator': 0, 'forward_tables': 0, 'updateposoutlettable': 1, 'upcskusearch': 1}
configuration class
2018-09-06 17:00:11,995 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 166 this is the begining of application
2018-09-06 17:00:11,996 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 167 adding all the project directories
2018-09-06 17:00:11,996 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 171 stages are: ['historysearch', 'writebackresults', 'updatehivetable', 'write_to_hdfs', 'creatingmap', 'newadditempoiddetection', 'generate_map_on_latest_data', 'loadodsitem', 'transfer_data_to_hdfs', 'loadposoutlet', 'updateodsitemtable', 'newaddmapping', 'partitioncreator', 'forward_tables', 'updateposoutlettable', 'upcskusearch']
Usage: snapshotdetector number of days0
recalculated  number of partition 15.0
15
09-06-2018 16:12:52
['hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00392.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00390.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00389.bz2']
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00392.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00390.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00390.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00389.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00389.bz2
snapshot data count before businesswise filtering: 944373
recalculated  number of partition 1.0
upload itemIdWithPartition08.txt file
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/itemidMapper/itemIdWithPartition08.txt
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/partitionList/partition_list.txt
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/fileSource/partition_list.txt hdfs:////npd/s_test2/partitionList/
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/maps/itemIdWithPartition08.txt hdfs:////npd/s_test2/itemidMapper/
0 and  and 
Reading itemId meta data
getting the last two columns
creating a temptable
Getting distinct partition names
Performing the join operation
snapshot data count: 4260
snapshot history data count: 3125
getting the list of files
restructuring the files
End of file processing.
End of the function
recalculated  number of partition 32.0
Usage: snapshotdetector number of days0
recalculated  number of partition 15.0
15
09-06-2018 16:09:24
['hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00394.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00392.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00391.bz2']
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00394.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00392.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00392.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00391.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00391.bz2
snapshot data count before businesswise filtering: 1099488
recalculated  number of partition 1.0
snapshot data count: 122439
same data count for identical RDD
getting the list of files
recalculated  number of partition 3.0
Start detecting the new adds
recalculated  number of partition 15.0
15
recalculated  number of partition 1.0
New add itemids count : 548
recalculated  number of partition 1.0
Filtered new poiid count 803
Start of remap detection for new add itemids and poiids 
recalculated  number of partition 1.0
recalculated  number of partition 1.0
Remaps count for itemid: 121891
Remaps count for poiid: 3457
End of the remap detection for new add itemids and poiids
start of filtering out the remaps based on attributes
count of actual remapsItemid after filtering: 121891
Count of the actual remapsPoiid after Filtering: 3457
End of the filtering out remaps based on attributes
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids










Initiate UPC, SKU and MODEL search operation
size of the usm results after filtering: 275
Size of sku search data: 528
Size of model search data: 400
Registering table for query execution
amount of data need to be searched with SKU: 528
size of original new add poiid 803
size of original new add poiid with valid upc value: 275
UPC search data displayed
Count of None or NULL value upc: 275
display of model search based data
Detecting records with proper UPC value
number of records Null upc count: 528
Initiate UPC based search to retrieve relevant itemids
Processing UPC Search
Root directory must be the part of configuration file
DataFrame[ITEMID: bigint, OUTLETBRAND: string, OUTLETITEMNUMBER: string, POIID: bigint, POSOUTLET: bigint, SKU: string, UPC: bigint, ITEM_ID: bigint]
[('ITEMID', 'bigint'), ('OUTLETBRAND', 'string'), ('OUTLETITEMNUMBER', 'string'), ('POIID', 'bigint'), ('POSOUTLET', 'bigint'), ('SKU', 'string'), ('UPC', 'bigint'), ('ITEM_ID', 'bigint')]
+--------------+---------+-----+-----+
|           UPC|   ITEMID|COUNT|TOTAL|
+--------------+---------+-----+-----+
|           UPC|   ITEMID|COUNT|TOTAL|
|  000029798407|230082206|    1|    1|
|00014215456385|220006795|    6|    6|
|  000194714929|230116571|    1|    1|
|00026585055334|220007725|    1|    2|
|00026585055334|220007724|    1|    2|
|00028914232522|240089068|    4|    4|
|00028914266817|240093459|    4|    4|
|00029757132512|230024759|    4|    4|
|00030317117429|230009029|    4|    4|
|00034223251970|230113205|    7|    7|
|00034223555719|230135589|    3|    3|
|00035011874227|240082502|    4|    4|
|00035011909653|240083590|    1|    1|
|00035011927220|240084381|    4|    4|
|00041969459621|220007828|    6|    6|
|00041969998755|220000591|    7|    7|
|00046928562015|230003877|    5|    5|
|00053242271295|230137687|   13|   13|
|00053474046654|230160577|    2|    2|
+--------------+---------+-----+-----+
only showing top 20 rows

[('UPC', 'string'), ('ITEMID', 'string'), ('COUNT', 'string'), ('TOTAL', 'string')]
[('ITEMID', 'bigint'), ('OUTLETBRAND', 'string'), ('OUTLETITEMNUMBER', 'string'), ('POIID', 'bigint'), ('POSOUTLET', 'bigint'), ('SKU', 'string'), ('UPC', 'bigint'), ('ITEM_ID', 'bigint')]
Number of upc based itemid retrieved: 19
after upc-itemid mapping inverse sku based itemid counts 206
Start of SKU based search operation
Processing SKU Search
Root directory must be the part of configuration file
DataFrame[ITEMID: bigint, OUTLETBRAND: string, OUTLETITEMNUMBER: string, POIID: bigint, POSOUTLET: bigint, SKU: string, UPC: bigint, ITEM_ID: bigint]
[('ITEMID', 'bigint'), ('OUTLETBRAND', 'string'), ('OUTLETITEMNUMBER', 'string'), ('POIID', 'bigint'), ('POSOUTLET', 'bigint'), ('SKU', 'string'), ('UPC', 'bigint'), ('ITEM_ID', 'bigint')]
+----------+---------+-----+-----+
|OUTLET_SKU|   ITEMID|COUNT|TOTAL|
+----------+---------+-----+-----+
|OUTLET_SKU|   ITEMID|COUNT|TOTAL|
|    129058|250904634|    1|    1|
|    129059|250907227|    1|    1|
|    129059|250880409|    1|    1|
|    129059|250911212|    1|    1|
|    129059|250905190|    3|    3|
|    129059|250905628|    3|    3|
|    129060|250904634|    2|    2|
|    129060|250905490|    3|    3|
|    129061|250905499|    1|    1|
|    216925|250899139|    1|    4|
|    216925|251159281|    1|    4|
|    216925|250946255|    1|    4|
|    216925|250880959|    1|    4|
|    216925|230154564|    1|    1|
|    216925|250909831|    1|    1|
|    216925|250985152|    1|    3|
|    216925|250898656|    1|    3|
|    216925|250924989|    1|    3|
|    216925|250951155|    2|    2|
+----------+---------+-----+-----+
only showing top 20 rows

[('OUTLET_SKU', 'string'), ('ITEMID', 'string'), ('COUNT', 'string'), ('TOTAL', 'string')]
[('ITEMID', 'bigint'), ('OUTLETBRAND', 'string'), ('OUTLETITEMNUMBER', 'string'), ('POIID', 'bigint'), ('POSOUTLET', 'bigint'), ('SKU', 'string'), ('UPC', 'bigint'), ('ITEM_ID', 'bigint')]
count of search list :334
Lenght of sku_list: 0
perform union of df_upc and sku_upc
lenght of upc and sku combine list 11
list of itemids
[u'230154567', u'220001715', u'251211576', u'230154554', u'230133938', u'230132832', u'230147630', u'230036127', u'230159641', u'251198796', u'250885811']
Initiate the sql Query
End of the sql Query
Write back to hdfs storage in csv format
Get the filtered files to read
Start detecting overlap data and return uniques
End of overlap detection of given snapshot rdd
Type cast the updated and added date column
read the base dictionary
perform left anti join on itemid to retrieve unique itemid based records
Update lastfile read number
last file name: part-m-00394.bz2
Start writing back to hive table
Get a hive write back object to write backe to hdfs
Set the table name
Executing command: set hive.support.concurrency=true
Executing command: set hive.enforce.bucketing=true
Executing command: set hive.compactor.initiator.on=true
Executing command: set hive.compactor.worker.threads= 10
Executing command: set hive.exec.dynamic.partition.mode=nonstrict
Executing command: set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
Executing command: set hive.tez.java.opts=-Xmx8192m
Executing command: set hive.vectorized.execution.enabled = true
Executing command: set hive.vectorized.execution.reduce.enabled = true
End of hive transfer
add partitions to snapshot data
perform leftanti join
total rdd results size after filter we need to distribute these items among different partition: 1135
Start detecting overlap data and return uniques
Initiate overlap detection
End of overlap detection
End of detecting the overlap data and return the uniques
start seperating the zero itemids
start seperating the zero itemid for base dictionary
End of seperating the zero itemids for base dictionary
type cast the updated and added date column
read the base dictionary
perform left anti join on poi_id to retrieve unique poi_id based records
Repartition the base dictionary data before start writing
Start writing back to hive table
Update lastfile read number in configuration file
last file name: part-m-00392.bz2
updating itemIdWithPartition file with new information
start type casting for date columns
end of type casting for date columns
Get a hive write back object to write backe to hdfs
Set the table name
Executing command: set hive.support.concurrency=true
Executing command: set hive.enforce.bucketing=true
Executing command: set hive.compactor.initiator.on=true
Executing command: set hive.compactor.worker.threads= 10
Executing command: set hive.exec.dynamic.partition.mode=nonstrict
Executing command: set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
Executing command: set hive.tez.java.opts=-Xmx8192m
Executing command: set hive.vectorized.execution.enabled = true
Executing command: set hive.vectorized.execution.reduce.enabled = true
End of transfer of data into hive internal table
Table been updated and written back successfully into hive
Start transfering the data from internal table to hdfs
Executing command: select itemid, businessid, subcategoryn, itemnumber, unitspackage,fld01, fld02, fld03, fld04, fld05, fld06, fld07, fld08, fld09, fld10, fld11, fld12, fld13, fld14, fld15, fld16, fld17, fld18, fld19, fld20, fld21, fld22, fld23, fld24, fld25, fld26, fld27, fld28, fld29, fld30, fld31, fld32, fld33, fld34, fld35, fld36, fld37, fld38, fld39, fld40, fld41, fld42, fld43, fld44, fld45, fld46, fld47, fld48, fld49, fld50, fld51, fld52, fld53, fld54, fld55, fld56, fld57, fld58, fld59, fld60, fld61, fld62, fld63, fld64, fld65, fld66, fld67, fld68, fld69, fld70, fld71, fld72, fld73, fld74, fld75, fld76, fld77, fld78, fld79, fld80, fld81, fld82, fld83, fld84, fld85, fld86, fld87, fld88, fld89, fld90, fld91, fld92, fld93, fld94, fld95, fld96, fld97, fld98, fld99, status, added, updated, vfld01, vfld02, vfld03, vfld04, vfld05, country_code, groupitemid, parentitemid, parentitemid_status, outletitem_map_change_date, lockdown_status from dqdictionaryhivedb.uniqueodsitems_int_limit_bs
start generating the maps from odsposoutlet table
start writing down the dictionary in hdfs
/npd/s_test2/uniqueOdsitemsMod
End of transfering the data
Start transfering the data from internal table to hdfs
Executing command: select poi_id ,business_id ,posoutlet ,outletdivision  ,outletdepartment ,outletsubdepartment ,outletclass ,outletsubclass ,outletbrand ,outletitemnumber,outletdescription ,outletbrandmatch ,outletitemnumbermatch ,outletdescriptionmatch ,sku ,manufacturercodetype ,manufacturercode ,zzzppmonthfrom ,zzzppmonthto , zzzppmonthlastused ,itemid ,itemtype ,price ,manufacturercodestatus ,loadid ,status ,added ,updated,ppweekfrom ,ppweekto ,ppweeklastused ,matched_country_code ,previous_poiid ,include_data_ppmonthfrom ,include_data_ppweekfrom ,manufacturercodematch ,skumatch , unitofmeasure ,packsize ,manufacturername ,manufacturernamematch ,privatelabel ,outletdescriptionsupplement ,total_confidence_score , parent_poiid ,parent_poiid_status, partitioner from dqdictionaryhivedb.uniqueodsposoutlet_int_limit_bs
start generating the maps from odsposoutlet table
start writing down the dictionary in hdfs
writing into: /npd/s_test2/uniqueBasedictionaryMod
End of transfering the data
End of Successful generation and write down of maps
End of the successfull operation
Ivy Default Cache set to: /home/abdul.sabbir/.ivy2/cache
The jars for the packages stored in: /home/abdul.sabbir/.ivy2/jars
:: loading settings :: url = jar:file:/usr/hdp/2.6.3.0-235/spark2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
mysql#mysql-connector-java added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found mysql#mysql-connector-java;5.1.39 in central
:: resolution report :: resolve 214ms :: artifacts dl 6ms
	:: modules in use:
	mysql#mysql-connector-java;5.1.39 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 1 already retrieved (0kB/6ms)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:ERROR Could not instantiate class [INFO].
java.lang.ClassNotFoundException: INFO
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Could not instantiate class [WARNING].
java.lang.ClassNotFoundException: WARNING
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:WARN Failed to set property [filter] to value "org.apache.log4j.varia.LevelRangeFilter". 
log4j:WARN No such property [datePattern] in org.apache.log4j.FileAppender.
log4j:ERROR Could not instantiate class [DEBUG].
java.lang.ClassNotFoundException: DEBUG
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)
	at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)
	at org.apache.log4j.PropertyConfigurator.parseAppenderFilters(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:845)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:186)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$$anonfun$3.apply(SparkSubmit.scala:325)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:325)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
main function for this pyspark project is going to be initiated soon .....
current working directory is :/hdpdata/pysparkProject/dqMachine
Operating system name : Linux
Machine Name : lpwhdqdnp01.npd.com
operating system version : 3.10.0-693.5.2.el7.x86_64
last access time #1 SMP Fri Oct 13 10:46:25 EDT 2017
operating system architecture : x86_64
Usage: snapshot detector number of days: 0
2018-09-07 06:49:01,000 [Thread-3] INFO  org.apache.spark.SparkContext - Running Spark version 2.2.0.2.6.3.0-235
2018-09-07 06:49:01,265 [Thread-3] WARN  org.apache.spark.SparkConf - In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
2018-09-07 06:49:01,274 [Thread-3] INFO  org.apache.spark.SparkContext - Submitted application: dqMachineApplication
2018-09-07 06:49:01,344 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-09-07 06:49:01,345 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-09-07 06:49:01,346 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-09-07 06:49:01,347 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-09-07 06:49:01,348 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-09-07 06:49:01,629 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 35357.
2018-09-07 06:49:01,645 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2018-09-07 06:49:01,663 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2018-09-07 06:49:01,674 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-09-07 06:49:01,674 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2018-09-07 06:49:01,740 [Thread-3] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /hdpdata/tmp/blockmgr-50c0655c-51d3-410a-a595-a8bc7594778a
2018-09-07 06:49:01,761 [Thread-3] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 5.2 GB
2018-09-07 06:49:01,866 [Thread-3] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2018-09-07 06:49:01,928 [Thread-3] INFO  org.spark_project.jetty.util.log - Logging initialized @7431ms
2018-09-07 06:49:01,977 [Thread-3] INFO  org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
2018-09-07 06:49:01,989 [Thread-3] INFO  org.spark_project.jetty.server.Server - Started @7493ms
2018-09-07 06:49:02,004 [Thread-3] INFO  org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@4072f048{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-09-07 06:49:02,006 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2018-09-07 06:49:02,026 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63f6b792{/jobs,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,026 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1d878abf{/jobs/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,027 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24d1c872{/jobs/job,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,028 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c08d07a{/jobs/job/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,029 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@364c290b{/stages,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,029 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50ffcb23{/stages/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,030 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31e4c7d0{/stages/stage,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,031 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b602640{/stages/stage/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,032 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46639f21{/stages/pool,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,032 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d05028a{/stages/pool/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,033 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c348d98{/storage,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,033 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@51160ab{/storage/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,034 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5fd0a2c2{/storage/rdd,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,035 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@686daead{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,035 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@200c7a30{/environment,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,036 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f4b31ee{/environment/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,036 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@463780fb{/executors,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,037 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2dc85420{/executors/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,037 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fdf271b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,038 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2da9bcab{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,042 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46aa3ae{/static,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,043 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26c16cf6{/,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,044 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40d27721{/api,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,045 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2345d34{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,045 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@111a234e{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-09-07 06:49:02,046 [Thread-3] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.231.1.14:4040
2018-09-07 06:49:02,153 [Thread-3] WARN  org.apache.spark.scheduler.FairSchedulableBuilder - Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
2018-09-07 06:49:02,155 [Thread-3] INFO  org.apache.spark.scheduler.FairSchedulableBuilder - Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
2018-09-07 06:49:02,904 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Looking for the active RM in [rm1, rm2]...
2018-09-07 06:49:02,957 [Thread-3] INFO  org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider - Found active RM [rm1]
2018-09-07 06:49:02,958 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Requesting a new application from cluster with 10 NodeManagers
2018-09-07 06:49:02,997 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Verifying our application has not requested more than the maximum memory capability of the cluster (196608 MB per container)
2018-09-07 06:49:02,997 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Will allocate AM container, with 896 MB memory including 384 MB overhead
2018-09-07 06:49:02,998 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up container launch context for our AM
2018-09-07 06:49:03,002 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Setting up the launch environment for our AM container
2018-09-07 06:49:03,008 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Preparing resources for our AM container
2018-09-07 06:49:03,850 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-09-07 06:49:03,853 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Source and destination file systems are the same. Not copying hdfs://hdqprod/hdp/apps/2.6.3.0-235/spark2/spark2-hdp-yarn-archive.tar.gz
2018-09-07 06:49:03,903 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16955/mysql_mysql-connector-java-5.1.39.jar
2018-09-07 06:49:04,132 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16955/pyspark.zip
2018-09-07 06:49:04,159 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16955/py4j-0.10.4-src.zip
2018-09-07 06:49:04,182 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/pysparkProject/dqMachine/dq_machine.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16955/dq_machine.zip
2018-09-07 06:49:04,217 [Thread-3] WARN  org.apache.spark.deploy.yarn.Client - Same path resource file:/home/abdul.sabbir/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.
2018-09-07 06:49:04,312 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Uploading resource file:/hdpdata/tmp/spark-4722fe3d-420d-4fa4-bd65-3c46c59d3341/__spark_conf__3237361277193801957.zip -> hdfs://hdqprod/user/abdul.sabbir/.sparkStaging/application_1524081153081_16955/__spark_conf__.zip
2018-09-07 06:49:04,348 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls to: abdul.sabbir
2018-09-07 06:49:04,348 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls to: abdul.sabbir
2018-09-07 06:49:04,348 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2018-09-07 06:49:04,348 [Thread-3] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2018-09-07 06:49:04,349 [Thread-3] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(abdul.sabbir); groups with view permissions: Set(); users  with modify permissions: Set(abdul.sabbir); groups with modify permissions: Set()
2018-09-07 06:49:04,354 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Submitting application application_1524081153081_16955 to ResourceManager
2018-09-07 06:49:04,379 [Thread-3] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1524081153081_16955
2018-09-07 06:49:04,382 [Thread-3] INFO  org.apache.spark.scheduler.cluster.SchedulerExtensionServices - Starting Yarn extension services with app application_1524081153081_16955 and attemptId None
2018-09-07 06:49:05,388 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16955 (state: ACCEPTED)
2018-09-07 06:49:05,392 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1536317344363
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16955/
	 user: abdul.sabbir
2018-09-07 06:49:06,393 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16955 (state: ACCEPTED)
2018-09-07 06:49:07,236 [dispatcher-event-loop-38] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lpwhdqnnp01.npd.com,lpwhdqnnp02.npd.com, PROXY_URI_BASES -> http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16955,http://lpwhdqnnp02.npd.com:8088/proxy/application_1524081153081_16955), /proxy/application_1524081153081_16955
2018-09-07 06:49:07,238 [dispatcher-event-loop-38] INFO  org.apache.spark.ui.JettyUtils - Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2018-09-07 06:49:07,395 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16955 (state: ACCEPTED)
2018-09-07 06:49:07,515 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
2018-09-07 06:49:08,397 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - Application report for application_1524081153081_16955 (state: RUNNING)
2018-09-07 06:49:08,398 [Thread-3] INFO  org.apache.spark.deploy.yarn.Client - 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.231.1.18
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1536317344363
	 final status: UNDEFINED
	 tracking URL: http://lpwhdqnnp01.npd.com:8088/proxy/application_1524081153081_16955/
	 user: abdul.sabbir
2018-09-07 06:49:08,399 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - Application application_1524081153081_16955 has started running.
2018-09-07 06:49:08,407 [Thread-3] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45379.
2018-09-07 06:49:08,408 [Thread-3] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 10.231.1.14:45379
2018-09-07 06:49:08,411 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-09-07 06:49:08,412 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.231.1.14, 45379, None)
2018-09-07 06:49:08,417 [dispatcher-event-loop-3] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 10.231.1.14:45379 with 5.2 GB RAM, BlockManagerId(driver, 10.231.1.14, 45379, None)
2018-09-07 06:49:08,420 [Thread-3] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.231.1.14, 45379, None)
2018-09-07 06:49:08,420 [Thread-3] INFO  org.apache.spark.storage.BlockManager - external shuffle service port = 7447
2018-09-07 06:49:08,421 [Thread-3] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 10.231.1.14, 45379, None)
2018-09-07 06:49:08,596 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@495cf59d{/metrics/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:08,766 [Thread-3] INFO  org.apache.spark.scheduler.EventLoggingListener - Logging events to file:/hdpdata/logs/application_1524081153081_16955
2018-09-07 06:49:11,264 [dispatcher-event-loop-34] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.18:48004) with ID 8
2018-09-07 06:49:11,317 [dispatcher-event-loop-26] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp05.npd.com:38542 with 4.1 GB RAM, BlockManagerId(8, lpwhdqdnp05.npd.com, 38542, None)
2018-09-07 06:49:11,484 [dispatcher-event-loop-35] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.22:37574) with ID 4
2018-09-07 06:49:11,536 [dispatcher-event-loop-28] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp09.npd.com:39313 with 4.1 GB RAM, BlockManagerId(4, lpwhdqdnp09.npd.com, 39313, None)
2018-09-07 06:49:11,611 [dispatcher-event-loop-21] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.19:45290) with ID 1
2018-09-07 06:49:11,659 [dispatcher-event-loop-29] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.21:57700) with ID 9
2018-09-07 06:49:11,660 [dispatcher-event-loop-2] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp06.npd.com:36251 with 4.1 GB RAM, BlockManagerId(1, lpwhdqdnp06.npd.com, 36251, None)
2018-09-07 06:49:11,685 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.20:34308) with ID 2
2018-09-07 06:49:11,707 [dispatcher-event-loop-15] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp08.npd.com:42489 with 4.1 GB RAM, BlockManagerId(9, lpwhdqdnp08.npd.com, 42489, None)
2018-09-07 06:49:11,718 [dispatcher-event-loop-10] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.17:43562) with ID 6
2018-09-07 06:49:11,732 [dispatcher-event-loop-17] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp07.npd.com:34031 with 4.1 GB RAM, BlockManagerId(2, lpwhdqdnp07.npd.com, 34031, None)
2018-09-07 06:49:11,773 [dispatcher-event-loop-14] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp04.npd.com:44625 with 4.1 GB RAM, BlockManagerId(6, lpwhdqdnp04.npd.com, 44625, None)
2018-09-07 06:49:11,795 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.23:49514) with ID 10
2018-09-07 06:49:11,817 [dispatcher-event-loop-6] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.16:44444) with ID 3
2018-09-07 06:49:11,836 [dispatcher-event-loop-13] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp10.npd.com:34005 with 4.1 GB RAM, BlockManagerId(10, lpwhdqdnp10.npd.com, 34005, None)
2018-09-07 06:49:11,869 [dispatcher-event-loop-31] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp03.npd.com:44460 with 4.1 GB RAM, BlockManagerId(3, lpwhdqdnp03.npd.com, 44460, None)
2018-09-07 06:49:11,906 [dispatcher-event-loop-33] INFO  org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.231.1.15:33568) with ID 7
2018-09-07 06:49:11,924 [Thread-3] INFO  org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2018-09-07 06:49:11,954 [dispatcher-event-loop-25] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager lpwhdqdnp02.npd.com:37555 with 4.1 GB RAM, BlockManagerId(7, lpwhdqdnp02.npd.com, 37555, None)
2018-09-07 06:49:12,017 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - loading hive config file: file:/etc/spark2/2.6.3.0-235/0/hive-site.xml
2018-09-07 06:49:12,036 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/').
2018-09-07 06:49:12,037 [Thread-3] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/'.
2018-09-07 06:49:12,042 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@397003a9{/SQL,null,AVAILABLE,@Spark}
2018-09-07 06:49:12,042 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b64d0f9{/SQL/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:12,043 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6731cf89{/SQL/execution,null,AVAILABLE,@Spark}
2018-09-07 06:49:12,044 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@669ac1c5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-09-07 06:49:12,045 [Thread-3] INFO  org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@53a1afb8{/static/sql,null,AVAILABLE,@Spark}
2018-09-07 06:49:12,371 [Thread-3] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-09-07 06:49:12,858 [Thread-3] INFO  hive.metastore - Trying to connect to metastore with URI thrift://lpwhdqnnp01.npd.com:9083
2018-09-07 06:49:12,907 [Thread-3] INFO  hive.metastore - Connected to metastore.
2018-09-07 06:49:13,089 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/161ccb90-050e-412a-8089-39f4a9e3c36d_resources
2018-09-07 06:49:13,092 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/161ccb90-050e-412a-8089-39f4a9e3c36d
2018-09-07 06:49:13,103 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/161ccb90-050e-412a-8089-39f4a9e3c36d
2018-09-07 06:49:13,105 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/161ccb90-050e-412a-8089-39f4a9e3c36d/_tmp_space.db
2018-09-07 06:49:13,107 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-09-07 06:49:13,317 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/60e28c13-a9f4-4b3a-b987-0d643b26aeb4_resources
2018-09-07 06:49:13,320 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/60e28c13-a9f4-4b3a-b987-0d643b26aeb4
2018-09-07 06:49:13,321 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created local directory: /tmp/abdul.sabbir/60e28c13-a9f4-4b3a-b987-0d643b26aeb4
2018-09-07 06:49:13,323 [Thread-3] INFO  org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/abdul.sabbir/60e28c13-a9f4-4b3a-b987-0d643b26aeb4/_tmp_space.db
2018-09-07 06:49:13,324 [Thread-3] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/hdpdata/pysparkProject/dqMachine/spark-warehouse/
2018-09-07 06:49:13,427 [Thread-3] INFO  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
mysql
hivedb
['dictionary_frontend', 'PACEDICTIONARY']
['dqdictionaryhivedb']
['REFS']
['DICTIONARY_STATUS', 'POIID_DETAILS', 'TIMETRACKER']
['uniqueodsposoutlet2_int', 'uniqueodsitems2_int', 'uniqueodsposoutlet_int_limit_bs', 'uniqueodsitems_int_limit_bs']
set([160, 161, 162, 174])
{'historysearch': 1, 'writebackresults': 1, 'updatehivetable': 1, 'write_to_hdfs': 0, 'creatingmap': 1, 'newadditempoiddetection': 1, 'generate_map_on_latest_data': 0, 'loadodsitem': 1, 'transfer_data_to_hdfs': 1, 'loadposoutlet': 1, 'updateodsitemtable': 1, 'newaddmapping': 1, 'partitioncreator': 0, 'forward_tables': 0, 'updateposoutlettable': 1, 'upcskusearch': 1}
configuration class
2018-09-07 06:49:13,536 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 166 this is the begining of application
2018-09-07 06:49:13,537 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 167 adding all the project directories
2018-09-07 06:49:13,537 [Thread-3] DEBUG myLogger_  - inside mainknitter module func mainOps line: 171 stages are: ['historysearch', 'writebackresults', 'updatehivetable', 'write_to_hdfs', 'creatingmap', 'newadditempoiddetection', 'generate_map_on_latest_data', 'loadodsitem', 'transfer_data_to_hdfs', 'loadposoutlet', 'updateodsitemtable', 'newaddmapping', 'partitioncreator', 'forward_tables', 'updateposoutlettable', 'upcskusearch']
Usage: snapshotdetector number of days0
recalculated  number of partition 15.0
15
09-06-2018 16:12:52
['hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00408.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00406.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00405.bz2']
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00408.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00406.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00406.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00405.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSOUTLETITEMS/part-m-00405.bz2
snapshot data count before businesswise filtering: 14217795
recalculated  number of partition 1.0
upload itemIdWithPartition08.txt file
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/itemidMapper/itemIdWithPartition08.txt
Running system command: hdfs dfs -rm hdfs:////npd/s_test2/partitionList/partition_list.txt
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/fileSource/partition_list.txt hdfs:////npd/s_test2/partitionList/
Running system command: hdfs dfs -copyFromLocal src/main/python/dictionary/maps/itemIdWithPartition08.txt hdfs:////npd/s_test2/itemidMapper/
0 and  and 
Reading itemId meta data
getting the last two columns
creating a temptable
Getting distinct partition names
Performing the join operation
snapshot data count: 128074
snapshot history data count: 126517
getting the list of files
restructuring the files
End of file processing.
End of the function
recalculated  number of partition 32.0
Usage: snapshotdetector number of days0
recalculated  number of partition 15.0
15
09-06-2018 16:09:24
['hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00410.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00408.bz2', 'hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00407.bz2']
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00410.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00408.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00408.bz2
File name: hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00407.bz2
File name :hdfs://lpwhdqnnp01.npd.com/npd/ODS/ODS_INPUTS_BZ2/ODS_POSITEMS/part-m-00407.bz2
snapshot data count before businesswise filtering: 5141
recalculated  number of partition 1.0
snapshot data count: 139
same data count for identical RDD
getting the list of files
recalculated  number of partition 3.0
Start detecting the new adds
recalculated  number of partition 15.0
15
recalculated  number of partition 1.0
New add itemids count : 132
recalculated  number of partition 1.0
Filtered new poiid count 106
Start of remap detection for new add itemids and poiids 
recalculated  number of partition 1.0
recalculated  number of partition 1.0
Remaps count for itemid: 7
Remaps count for poiid: 127968
End of the remap detection for new add itemids and poiids
start of filtering out the remaps based on attributes
count of actual remapsItemid after filtering: 7
Count of the actual remapsPoiid after Filtering: 127968
End of the filtering out remaps based on attributes
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids
Start of the finding the ref poiids
Start filtering out the extraneous ref itemids
End of the filtering out the extraneous ref itemids
End of the finding the ref poiids


Initiate UPC, SKU and MODEL search operation
Traceback (most recent call last):
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/main.py", line 90, in <module>
    mainOb.executor()
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/main.py", line 78, in executor
    mainOps()
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/dictionary/mainKnitter.py", line 256, in mainOps
    listOfItemids, dataFrameItemId, business_rdd = find_candidate_itemids(sc = gv.spark_context, itemid_f = itemid_frame,poiid_f = poiid_frame, config_ = configOb) if configOb.stage['upcskusearch'] is not 0 else (None, sqlc.createDataFrame(gv.spark_context.emptyRDD(), StructType([])), sqlc.createDataFrame(gv.spark_context.emptyRDD(), StructType([]))) # upc_m = upc_map,sku_m = sku_map,model_m = model_map) # related itemid retriever 
  File "/hdpdata/pysparkProject/dqMachine/src/main/python/dictionary/upcModule/find_candidate_itemids.py", line 106, in find_candidate_itemids
    usm_results = poiid_f.rdd.map(lambda x : Row(POIID = x.poi_id, UPC = x.manufacturercode, SKU = x.sku, POSOUTLET = x.posoutlet, OUTLETBRAND = x.outletbrand, OUTLETITEMNUMBER = x.outletitemnumber, ITEMID = x.itemid)).toDF()
  File "/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/session.py", line 57, in toDF
  File "/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/session.py", line 535, in createDataFrame
  File "/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/session.py", line 375, in _createFromRDD
  File "/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/session.py", line 362, in _inferSchema
ValueError: Some of types cannot be determined by the first 100 rows, please try again with sampling
